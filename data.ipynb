{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together(api_key='fafdfc532099c3e6a872111138b35c94de8b65dd6f810e635b2630e64f4b4d72')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temp=0.7):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \" Act like a expert in IA.\"},  \n",
    "        {\"role\": \"user\", \"content\": prompt}  \n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,  \n",
    "        max_tokens=None,\n",
    "        temperature=temp,\n",
    "        top_p=0.7,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1,\n",
    "        stop=[\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for token in response:\n",
    "      if hasattr(token, 'choices'):\n",
    "        print(token.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strcuture_data='''\n",
    "CREATE TABLE [dbo].[rpt_actual_casos](\n",
    "\t[IdCaso] [int] NOT NULL,\n",
    "\t[idTicket] [bigint] NULL,\n",
    "\t[idCliente] [bigint] NULL,\n",
    "\t[canal] [varchar](45) NULL,\n",
    "\t[estadoTicket] [int] NULL,\n",
    "\t[Estado] [varchar](50) NULL,\n",
    "\t[frecepcion] [smalldatetime] NULL,\n",
    "\t[fgestion] [smalldatetime] NULL,\n",
    "\t[fechaEjecutivo] [smalldatetime] NULL,\n",
    "\t[fechaTicket] [smalldatetime] NULL,\n",
    "\t[fechaCierreRequerimiento] [smalldatetime] NULL,\n",
    "\t[tiempoRespuesta_min] [int] NULL,\n",
    "\t[nombre_area] [varchar](200) NULL,\n",
    "\t[nombre_categoria] [varchar](500) NULL,\n",
    "\t[descripcionProceso] [varchar](100) NULL,\n",
    "\t[descripcionSubProceso] [varchar](100) NULL,\n",
    "\t[mensajeTicket] [ntext] NULL,\n",
    "\t[scriptFase] [nvarchar](100) NULL,\n",
    "\t[fechaAsignacion] [smalldatetime] NULL,\n",
    "\t[fechaResolucion] [smalldatetime] NULL,\n",
    "\t[mensajeResolucion] [ntext] NULL,\n",
    "\t[nombreCliente] [varchar](136) NOT NULL,\n",
    "\t[descripcionGrupo] [varchar](100) NULL,\n",
    "\t[PNR] [varchar](50) NULL,\n",
    "\t[Usuario_Creacion] [varchar](50) NULL,\n",
    "\t[Usuario_Cierre] [varchar](50) NULL,\n",
    "\t[T_horas_cierre]  AS (case when [fechaCierreRequerimiento] IS NULL then (0) else datediff(hour,[frecepcion],[fechaCierreRequerimiento]) end),\n",
    "\t[T_dias_cierre]  AS (case when [fechaCierreRequerimiento] IS NULL then (0) else datediff(day,[frecepcion],[fechaCierreRequerimiento]) end),\n",
    "\t[Usuario_Asignado] [varchar](50) NULL,\n",
    "\t[cod_gestion]  AS (case when [fechaTicket] IS NOT NULL AND [fgestion] IS NULL then 'T' when [fechaTicket] IS NOT NULL AND [fgestion] IS NOT NULL then 'GT' when [fechaTicket] IS NULL AND [fgestion] IS NULL then 'SN' else 'G' end),\n",
    "\t[T_horas_sin_gestion]  AS (case when [fechaCierreRequerimiento] IS NOT NULL then (0) else datediff(hour,[frecepcion],getdate()) end),\n",
    "\t[T_dias_sin_gestion]  AS (case when [fechaCierreRequerimiento] IS NOT NULL then (0) else datediff(day,[frecepcion],getdate()) end),\n",
    "\t[Rango_cierre]  AS (case when [fechaCierreRequerimiento] IS NULL then '' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<(1) then 'Menos de 1 hora' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])>=(1) AND datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(6) then '1 y 6 horas' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(12) then '6 y 12 horas' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(24) then '12 y 24 horas' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(48) then '24 y 48 horas' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(72) then '48 y 72 horas' when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(5) then '3 y 5 días' when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(10) then '6 y 10 días' when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(15) then '10 y 15 días' when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(20) then '15 y 20 días' when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(30) then '20 y 30 días' else 'Más de 30 días' end),\n",
    "\t[Rango_sin_gestion]  AS (case when [fechaCierreRequerimiento] IS NOT NULL then '' when datediff(hour,[frecepcion],getdate())>=(1) AND datediff(hour,[frecepcion],getdate())<=(6) then '1 y 6 horas' when datediff(hour,[frecepcion],getdate())>(6) AND datediff(hour,[frecepcion],getdate())<=(12) then '6 y 12 horas' when datediff(hour,[frecepcion],getdate())>(12) AND datediff(hour,[frecepcion],getdate())<=(24) then '12 y 24 horas' when datediff(hour,[frecepcion],getdate())>(24) AND datediff(hour,[frecepcion],getdate())<=(48) then '24 y 48 horas' when datediff(hour,[frecepcion],getdate())>(48) AND datediff(hour,[frecepcion],getdate())<=(72) then '48 y 72 horas' when datediff(day,[frecepcion],getdate())>=(3) AND datediff(day,[frecepcion],getdate())<=(5) then '3 y 5 días' when datediff(day,[frecepcion],getdate())>(5) AND datediff(day,[frecepcion],getdate())<=(10) then '6 y 10 días' when datediff(day,[frecepcion],getdate())>(10) AND datediff(day,[frecepcion],getdate())<=(15) then '10 y 15 días' when datediff(day,[frecepcion],getdate())>(15) AND datediff(day,[frecepcion],getdate())<=(20) then '15 y 20 días' when datediff(day,[frecepcion],getdate())>(20) AND datediff(day,[frecepcion],getdate())<=(30) then '20 y 30 días' when datediff(day,[frecepcion],getdate())>(30) then 'Más de 30 días' else 'Menos de 1 hora' end),\n",
    "\t[cod_rango_cierre]  AS (case when [fechaCierreRequerimiento] IS NULL then '' when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<(1) then (1) when datediff(hour,[frecepcion],[fechaCierreRequerimiento])>=(1) AND datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(6) then (2) when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(12) then (3) when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(24) then (4) when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(48) then (5) when datediff(hour,[frecepcion],[fechaCierreRequerimiento])<=(72) then (6) when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(5) then (7) when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(10) then (8) when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(15) then (9) when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(20) then (10) when datediff(day,[frecepcion],[fechaCierreRequerimiento])<=(30) then (11) else (12) end),\n",
    "\t[cod_rango_sin_gestion]  AS (case when [fechaCierreRequerimiento] IS NOT NULL then '' when datediff(hour,[frecepcion],getdate())>=(1) AND datediff(hour,[frecepcion],getdate())<=(6) then (2) when datediff(hour,[frecepcion],getdate())>(6) AND datediff(hour,[frecepcion],getdate())<=(12) then (3) when datediff(hour,[frecepcion],getdate())>(12) AND datediff(hour,[frecepcion],getdate())<=(24) then (4) when datediff(hour,[frecepcion],getdate())>(24) AND datediff(hour,[frecepcion],getdate())<=(48) then (5) when datediff(hour,[frecepcion],getdate())>(48) AND datediff(hour,[frecepcion],getdate())<=(72) then (6) when datediff(day,[frecepcion],getdate())>=(3) AND datediff(day,[frecepcion],getdate())<=(5) then (7) when datediff(day,[frecepcion],getdate())>(5) AND datediff(day,[frecepcion],getdate())<=(10) then (8) when datediff(day,[frecepcion],getdate())>(10) AND datediff(day,[frecepcion],getdate())<=(15) then (9) when datediff(day,[frecepcion],getdate())>(15) AND datediff(day,[frecepcion],getdate())<=(20) then (10) when datediff(day,[frecepcion],getdate())>(20) AND datediff(day,[frecepcion],getdate())<=(30) then (11) when datediff(day,[frecepcion],getdate())>(30) then (12) else (1) end),\n",
    "\t[idLlamada] [int] NULL\n",
    ") ON [PRIMARY] TEXTIMAGE_ON [PRIMARY]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''\n",
    "           Take this structure of data and build a datasets amplio with the follow \n",
    "            DatasetDict({\n",
    "                train: Dataset({\n",
    "                    features: ['answer', 'question', 'context'],\n",
    "                    num_rows: 78577\n",
    "                })\n",
    "            })\n",
    "include valid and test\n",
    "\n",
    "example: {'answer': 'SELECT Track AS title FROM table_name_6 WHERE year < 2005',\n",
    " 'question': 'Which track title has a year lesser thsn 2005?',\n",
    " 'context': 'CREATE TABLE table_name_6 (Track VARCHAR, year INTEGER)'}\n",
    "\n",
    "Structure Data: ```{strcuture_data}```           \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a comprehensive dataset with train, validation, and test sets based on the provided structure, we'll need to generate a variety of examples that cover different scenarios, including various SQL queries, questions, and contexts. The goal is to ensure the dataset is diverse and useful for training models in the context of SQL query generation from natural language questions.\n",
      "\n",
      "Below is a simplified approach to generating such a dataset. Note that creating a large, diverse dataset requires a systematic approach to covering various SQL operations (SELECT, INSERT, UPDATE, DELETE), different table structures, and a wide range of questions that could be asked about the data in those tables.\n",
      "\n",
      "```python\n",
      "from datasets import Dataset, DatasetDict\n",
      "\n",
      "# Example data points\n",
      "data_points = [\n",
      "    {'answer': 'SELECT Track AS title FROM table_name_6 WHERE year < 2005',\n",
      "     'question': 'Which track title has a year lesser than 2005?',\n",
      "     'context': 'CREATE TABLE table_name_6 (Track VARCHAR, year INTEGER)'},\n",
      "    {'answer': 'SELECT * FROM table_name_7 WHERE age > 18',\n",
      "     'question': 'What are all the details of individuals older than 18?',\n",
      "     'context': 'CREATE TABLE table_name_7 (name VARCHAR, age INTEGER, country VARCHAR)'},\n",
      "    {'answer': 'INSERT INTO table_name_8 (name, age) VALUES (\\'John Doe\\', 30)',\n",
      "     'question': 'How do you insert a new person named John Doe who is 30 years old into the table?',\n",
      "     'context': 'CREATE TABLE table_name_8 (name VARCHAR, age INTEGER)'},\n",
      "    # Add more examples here...\n",
      "]\n",
      "\n",
      "# Split the data into train, validation, and test sets\n",
      "# For simplicity, let's assume we split the data into 80% for training, 10% for validation, and 10% for testing\n",
      "import random\n",
      "random.shuffle(data_points)\n",
      "train_size = int(len(data_points) * 0.8)\n",
      "val_size = int(len(data_points) * 0.1)\n",
      "train_data = data_points[:train_size]\n",
      "val_data = data_points[train_size:train_size+val_size]\n",
      "test_data = data_points[train_size+val_size:]\n",
      "\n",
      "# Create datasets\n",
      "train_dataset = Dataset.from_list(train_data)\n",
      "val_dataset = Dataset.from_list(val_data)\n",
      "test_dataset = Dataset.from_list(test_data)\n",
      "\n",
      "# Create a DatasetDict\n",
      "dataset = DatasetDict({\n",
      "    \"train\": train_dataset,\n",
      "    \"validation\": val_dataset,\n",
      "    \"test\": test_dataset\n",
      "})\n",
      "\n",
      "print(dataset)\n",
      "```\n",
      "\n",
      "This example demonstrates how to create a simple dataset with train, validation, and test splits. However, for a real-world application, you would need to generate a much larger and more diverse set of examples to effectively train a model. This could involve programmatically generating SQL queries and corresponding natural language questions based on a variety of table schemas and data types.\n",
      "\n",
      "To amplify the dataset, consider the following strategies:\n",
      "- **Automated Generation**: Write scripts to generate SQL queries (SELECT, INSERT, UPDATE, DELETE) and corresponding questions based on predefined table structures.\n",
      "- **Data Augmentation**: For each SQL query and question pair, generate variations by changing the query slightly (e.g., changing the condition in a WHERE clause) and adjusting the question accordingly.\n",
      "- **Human Annotation**: Engage human annotators to review and expand the dataset, ensuring that the questions are natural and the SQL queries are correct and relevant.\n",
      "- **Use Existing Resources**: Leverage existing datasets and resources related to SQL and natural language processing to expand your dataset.\n",
      "\n",
      "Remember, the quality and diversity of your dataset are crucial for training effective models in the context of IA (Intelligence Artificielle or Artificial Intelligence).None\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "with open('./datasets/sql_text.json','r', encoding='utf-8') as file:\n",
    "    data_points=json.load(file)\n",
    "\n",
    "random.shuffle(data_points)\n",
    "train_size = int(len(data_points) * 0.8)\n",
    "val_size = int(len(data_points) * 0.1)\n",
    "train_data = data_points[:train_size]\n",
    "val_data = data_points[train_size:train_size+val_size]\n",
    "test_data = data_points[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answer', 'question', 'context'],\n",
      "        num_rows: 64000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answer', 'question', 'context'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answer', 'question', 'context'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 64000/64000 [00:00<00:00, 830000.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8000/8000 [00:00<00:00, 725830.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8000/8000 [00:00<00:00, 659391.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./datasets/sql_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
