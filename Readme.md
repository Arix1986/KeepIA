### GeneraciÃ³n de SQL desde Lenguaje Natural

#### DescripciÃ³n del Proyecto

Este proyecto es una Prueba de Concepto (PoC) que explora la posibilidad de transformar consultas en lenguaje natural a sentencias SQL mediante el ajuste fino (*fine-tuning*) de modelos de lenguaje avanzados, como **T5-Large** y **Facebook/MBART-Large-50**.

El objetivo es evaluar la efectividad de estos modelos para comprender el contexto de bases de datos y generar consultas SQL precisas en funciÃ³n de preguntas formuladas en lenguaje natural.

#### Estructura del Proyecto

```plaintext
ðŸ“‚ KeepIA
â”‚â”€â”€ ðŸ“‚ datasets        # Dataset generado con preguntas, respuestas y contexto SQL
â”‚   â”‚â”€â”€ sql_dataset       
â”‚   â”‚â”€â”€ sql_text.json     
â”‚
â”‚â”€â”€ ðŸ“‚ outputs
â”‚   â”‚â”€â”€ models/      # Checkpoints de modelos ajustados
â”‚
â”‚
â”‚â”€â”€ main.ipynb          # Clase principal con el flujo de la PoC
â”‚â”€â”€ data.ipynb          # Preprocesamiento de datos y generaciÃ³n del dataset
â”‚â”€â”€ README.md           # DocumentaciÃ³n del proyecto
â”‚â”€â”€ requirements.txt    # LibrerÃ­as necesarias 
```
### Detalles de los Archivos Principales

####  `data.ipynb` - Preprocesamiento de Datos

Este archivo contiene el proceso de preparaciÃ³n de datos para generar un dataset propio con la siguiente estructura:

```json
{
 'answer': "SELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso'",
 'question': "Â¿CuÃ¡l es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?",
 'context': "CREATE TABLE rpt_actual_casos.."
}
```
###  `main.ipynb` - Flujo de la Prueba de Concepto

Este cuaderno contiene la implementaciÃ³n principal del proyecto. AquÃ­ se realizan los siguientes pasos:

1. **Carga y preprocesamiento de datos**
   - Se utiliza `data.ipynb` para estructurar el dataset con preguntas en lenguaje natural, su respectiva consulta SQL y el esquema de la base de datos.
   
2. **Entrenamiento de los modelos**
   - Se realiza *fine-tuning* de los modelos `T5-Large` y `Facebook/MBART-Large-50` utilizando el dataset generado.
   - Se ajustan los hiperparÃ¡metros clave como `learning_rate`, `batch_size` y `num_train_epochs`.
   - Se almacena el modelo ajustado en la carpeta `models/checkpoint/`.

3. **GeneraciÃ³n de consultas SQL**
   - Se evalÃºa la capacidad de los modelos para convertir preguntas en consultas SQL utilizando `generate()` de Hugging Face.
   - Se compara la `model_response` con la `ground_truth` para medir la precisiÃ³n.

4. **EvaluaciÃ³n del desempeÃ±o**
   - Se calcula la `Training Loss` y `Validation Loss` para determinar la convergencia del modelo.
   - Se analiza la calidad de las consultas generadas y los errores mÃ¡s comunes.

---

###  Conclusiones y PrÃ³ximos Pasos

Los primeros resultados obtenidos indican que **los modelos aÃºn no generan consultas SQL **. Se identificaron algunas Ã¡reas de mejora:

- **Uso de datasets mÃ¡s especializados**  
  - Implementar **Spider**, un dataset ampliamente utilizado para entrenamiento en Text-to-SQL.
  - Explorar otros datasets que permitan diversificar los ejemplos de entrenamiento.

- **Refinamiento del preprocesamiento de datos**  
  - Asegurar que la estructura de la base de datos se incluya correctamente en el contexto.
  - Aumentar la cantidad de ejemplos con variaciones en las preguntas para mejorar la generalizaciÃ³n.

- **CalibraciÃ³n de hiperparÃ¡metros**  
  - Ajustar la tasa de aprendizaje (`learning_rate`).
  - Experimentar con distintas estrategias de generaciÃ³n, como `beam search` o `top-k sampling`.

El archivo JSON `sql_text` se encuentra disponible en **Google Drive** debido a su tamaÃ±o.  
Puedes acceder a Ã©l a travÃ©s del siguiente enlace:  

ðŸ”— [Descargar JSON `sql_text`](https://drive.google.com/drive/folders/1qiFGprasKba-LRbovRokvgHHIyPrOiKY?usp=sharing)
