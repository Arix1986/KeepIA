{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmWKj0NzGP5P"
   },
   "source": [
    "# Template del Prompt para el Trainig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQPD37SFGP5Q"
   },
   "outputs": [],
   "source": [
    "TEXT2SQL_TRAINING_PROMPT_TEMPLATE = \"\"\"\\\n",
    "### Instrucciones:\n",
    "{system_message}\n",
    "\n",
    "### Pregunta:\n",
    "{input}\n",
    "\n",
    "### Contexto:\n",
    "{context}\n",
    "\n",
    "### Respuesta:\n",
    "{response}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8KGEddwGP5R"
   },
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "    \"Eres un modelo especializado en convertir lenguaje natural a SQL. \"\n",
    "    \"Dada una pregunta en lenguaje natural y un contexto opcional de base de datos (esquema de tablas, tipos de datos, etc.), \"\n",
    "    \"genera una consulta SQL válida y eficiente. \"\n",
    "    \"Asegúrate de usar las columnas y tablas correctas, y evita consultas innecesariamente complejas.\"\n",
    "    \"Responde solo con la consulta SQL y no incluyas explicaciones adicionales.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPdvQqwGGP5R"
   },
   "outputs": [],
   "source": [
    "def create_sql_prompt(sample):\n",
    "  full_prompt = TEXT2SQL_TRAINING_PROMPT_TEMPLATE.format(\n",
    "      system_message = SYSTEM_MESSAGE,\n",
    "      input = sample[\"question\"],\n",
    "      context = sample[\"context\"],\n",
    "      response = sample[\"answer\"]\n",
    "  )\n",
    "\n",
    "  return {\"text\" : full_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIk2xgn4W7Xu"
   },
   "source": [
    "# Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2599,
     "status": "ok",
     "timestamp": 1740828721569,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "sk2S3M0bHeUP",
    "outputId": "1626e804-1a9b-4965-b637-7781bfaa1e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.3.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.48.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu124)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qufgLTUrGP5R"
   },
   "outputs": [],
   "source": [
    "from datasets import  DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2973,
     "status": "ok",
     "timestamp": 1740828725451,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "fc5lfnLhYRCK",
    "outputId": "79be052a-88e1-4eae-b0dc-55f1b3a620f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx8uoYA7GP5S"
   },
   "outputs": [],
   "source": [
    "df=DatasetDict.load_from_disk('/content/drive/MyDrive/IA/datasets/sql_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740828725541,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "EDftZ57IGP5S",
    "outputId": "fccb7d8d-e1cd-444d-97b5-713cb94eca74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"SELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso'\",\n",
       " 'question': \"¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\",\n",
       " 'context': 'CREATE TABLE rpt_actual_casos (\\n            IdCaso INT NOT NULL,\\n            idTicket BIGINT NULL,\\n            idCliente BIGINT NULL,\\n            canal VARCHAR(45) NULL,\\n            estadoTicket INT NULL,\\n            Estado VARCHAR(50) NULL,\\n            frecepcion TIMESTAMP NULL,\\n            fgestion TIMESTAMP NULL,\\n            fechaEjecutivo TIMESTAMP NULL,\\n            fechaTicket TIMESTAMP NULL,\\n            fechaCierreRequerimiento TIMESTAMP NULL,\\n            tiempoRespuesta_min INT NULL,\\n            nombre_area VARCHAR(200) NULL,\\n            nombre_categoria VARCHAR(500) NULL,\\n            descripcionProceso VARCHAR(100) NULL,\\n            descripcionSubProceso VARCHAR(100) NULL,\\n            mensajeTicket TEXT NULL,\\n            scriptFase NVARCHAR(100) NULL,\\n            fechaAsignacion TIMESTAMP NULL,\\n            fechaResolucion TIMESTAMP NULL,\\n            mensajeResolucion TEXT NULL,\\n            nombreCliente VARCHAR(136) NOT NULL,\\n            descripcionGrupo VARCHAR(100) NULL,\\n            PNR VARCHAR(50) NULL,\\n            Usuario_Creacion VARCHAR(50) NULL,\\n            Usuario_Cierre VARCHAR(50) NULL\\n        )'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740828725545,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "gTl3DI9LGP5T",
    "outputId": "91f6ff9b-65dd-4ea0-f905-911df66b26b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"### Instrucciones:\\nEres un modelo especializado en convertir lenguaje natural a SQL. Dada una pregunta en lenguaje natural y un contexto opcional de base de datos (esquema de tablas, tipos de datos, etc.), genera una consulta SQL válida y eficiente. Asegúrate de usar las columnas y tablas correctas, y evita consultas innecesariamente complejas.Responde solo con la consulta SQL y no incluyas explicaciones adicionales.\\n\\n### Pregunta:\\n¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\\n\\n### Contexto:\\nCREATE TABLE rpt_actual_casos (\\n            IdCaso INT NOT NULL,\\n            idTicket BIGINT NULL,\\n            idCliente BIGINT NULL,\\n            canal VARCHAR(45) NULL,\\n            estadoTicket INT NULL,\\n            Estado VARCHAR(50) NULL,\\n            frecepcion TIMESTAMP NULL,\\n            fgestion TIMESTAMP NULL,\\n            fechaEjecutivo TIMESTAMP NULL,\\n            fechaTicket TIMESTAMP NULL,\\n            fechaCierreRequerimiento TIMESTAMP NULL,\\n            tiempoRespuesta_min INT NULL,\\n            nombre_area VARCHAR(200) NULL,\\n            nombre_categoria VARCHAR(500) NULL,\\n            descripcionProceso VARCHAR(100) NULL,\\n            descripcionSubProceso VARCHAR(100) NULL,\\n            mensajeTicket TEXT NULL,\\n            scriptFase NVARCHAR(100) NULL,\\n            fechaAsignacion TIMESTAMP NULL,\\n            fechaResolucion TIMESTAMP NULL,\\n            mensajeResolucion TEXT NULL,\\n            nombreCliente VARCHAR(136) NOT NULL,\\n            descripcionGrupo VARCHAR(100) NULL,\\n            PNR VARCHAR(50) NULL,\\n            Usuario_Creacion VARCHAR(50) NULL,\\n            Usuario_Cierre VARCHAR(50) NULL\\n        )\\n\\n### Respuesta:\\nSELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso' \\n\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sql_prompt(df[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X79U6fLGP5T"
   },
   "outputs": [],
   "source": [
    "sql_dataset = df.map(create_sql_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740828725655,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "RSaxpZk9GP5U",
    "outputId": "5a24ac93-92bd-4448-ee10-e0cc9c51a6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"SELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso'\",\n",
       " 'question': \"¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\",\n",
       " 'context': 'CREATE TABLE rpt_actual_casos (\\n            IdCaso INT NOT NULL,\\n            idTicket BIGINT NULL,\\n            idCliente BIGINT NULL,\\n            canal VARCHAR(45) NULL,\\n            estadoTicket INT NULL,\\n            Estado VARCHAR(50) NULL,\\n            frecepcion TIMESTAMP NULL,\\n            fgestion TIMESTAMP NULL,\\n            fechaEjecutivo TIMESTAMP NULL,\\n            fechaTicket TIMESTAMP NULL,\\n            fechaCierreRequerimiento TIMESTAMP NULL,\\n            tiempoRespuesta_min INT NULL,\\n            nombre_area VARCHAR(200) NULL,\\n            nombre_categoria VARCHAR(500) NULL,\\n            descripcionProceso VARCHAR(100) NULL,\\n            descripcionSubProceso VARCHAR(100) NULL,\\n            mensajeTicket TEXT NULL,\\n            scriptFase NVARCHAR(100) NULL,\\n            fechaAsignacion TIMESTAMP NULL,\\n            fechaResolucion TIMESTAMP NULL,\\n            mensajeResolucion TEXT NULL,\\n            nombreCliente VARCHAR(136) NOT NULL,\\n            descripcionGrupo VARCHAR(100) NULL,\\n            PNR VARCHAR(50) NULL,\\n            Usuario_Creacion VARCHAR(50) NULL,\\n            Usuario_Cierre VARCHAR(50) NULL\\n        )',\n",
       " 'text': \"### Instrucciones:\\nEres un modelo especializado en convertir lenguaje natural a SQL. Dada una pregunta en lenguaje natural y un contexto opcional de base de datos (esquema de tablas, tipos de datos, etc.), genera una consulta SQL válida y eficiente. Asegúrate de usar las columnas y tablas correctas, y evita consultas innecesariamente complejas.Responde solo con la consulta SQL y no incluyas explicaciones adicionales.\\n\\n### Pregunta:\\n¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\\n\\n### Contexto:\\nCREATE TABLE rpt_actual_casos (\\n            IdCaso INT NOT NULL,\\n            idTicket BIGINT NULL,\\n            idCliente BIGINT NULL,\\n            canal VARCHAR(45) NULL,\\n            estadoTicket INT NULL,\\n            Estado VARCHAR(50) NULL,\\n            frecepcion TIMESTAMP NULL,\\n            fgestion TIMESTAMP NULL,\\n            fechaEjecutivo TIMESTAMP NULL,\\n            fechaTicket TIMESTAMP NULL,\\n            fechaCierreRequerimiento TIMESTAMP NULL,\\n            tiempoRespuesta_min INT NULL,\\n            nombre_area VARCHAR(200) NULL,\\n            nombre_categoria VARCHAR(500) NULL,\\n            descripcionProceso VARCHAR(100) NULL,\\n            descripcionSubProceso VARCHAR(100) NULL,\\n            mensajeTicket TEXT NULL,\\n            scriptFase NVARCHAR(100) NULL,\\n            fechaAsignacion TIMESTAMP NULL,\\n            fechaResolucion TIMESTAMP NULL,\\n            mensajeResolucion TEXT NULL,\\n            nombreCliente VARCHAR(136) NOT NULL,\\n            descripcionGrupo VARCHAR(100) NULL,\\n            PNR VARCHAR(50) NULL,\\n            Usuario_Creacion VARCHAR(50) NULL,\\n            Usuario_Cierre VARCHAR(50) NULL\\n        )\\n\\n### Respuesta:\\nSELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso' \\n\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDRUw58bGP5U"
   },
   "source": [
    "#### Cargar el modelo a afinar y preprocesarlo\n",
    "\n",
    "Usaremos mbart-large-50, que es multilingüe y T5 realizaremos fine-tuning en tareas de conversión de texto a SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12736,
     "status": "ok",
     "timestamp": 1740828738392,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "yH-fO645GP5U",
    "outputId": "3b3080b8-bc31-4a3b-d030-4fc8fbab8514"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,MBartForConditionalGeneration\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740828738456,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "jNiYk_7FGP5U",
    "outputId": "56b8e9d2-a100-418a-d5bf-02c902f276d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartScaledWordEmbedding(250054, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aWe3xo-JI0-"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GenerationConfig\n",
    "\n",
    "class Generador:\n",
    "    def __init__(self, model, tokenizer, seed=42):\n",
    "        self.TEXT2SQL_INFERENCE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "            ### Instrucciones:\n",
    "            {system_message}\n",
    "\n",
    "            ### Pregunta:\n",
    "            {input}\n",
    "\n",
    "            ### Contexto:\n",
    "            {context}\n",
    "\n",
    "            ### Respuesta:\n",
    "        \"\"\"\n",
    "        self.SYSTEM_MESSAGE = (\n",
    "            \"Eres un modelo especializado en convertir lenguaje natural a SQL. \"\n",
    "            \"Dada una pregunta en lenguaje natural y un contexto opcional de base de datos (esquema de tablas, tipos de datos, etc.), \"\n",
    "            \"genera una consulta SQL válida y eficiente. \"\n",
    "            \"Asegúrate de usar las columnas y tablas correctas, y evita consultas innecesariamente complejas.\"\n",
    "            \"Responde solo con la consulta SQL y no incluyas explicaciones adicionales.\"\n",
    "        )\n",
    "        set_seed(seed)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generator = pipeline('text2text-generation', model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def get_generation_config(self, max_tokens=512, top_k=512, temperature=1e-4):\n",
    "        return GenerationConfig(\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "            eos_token_id=self.model.config.eos_token_id,\n",
    "        )\n",
    "\n",
    "    def generate_sample(self, sample):\n",
    "        prompt_package = self.create_sql_prompt_and_response(sample)\n",
    "        config = self.get_generation_config()\n",
    "\n",
    "        # Generar la respuesta del modelo\n",
    "        generation = self.generator(prompt_package[\"full_prompt\"], generation_config=config)\n",
    "        generated_text = generation[0][\"generated_text\"]\n",
    "\n",
    "        # Intentar limpiar la salida\n",
    "        if generated_text.startswith(prompt_package[\"full_prompt\"]):\n",
    "            generated_text = generated_text[len(prompt_package[\"full_prompt\"]):].strip()\n",
    "\n",
    "        return {\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"ground_truth\": prompt_package[\"ground_truth\"],\n",
    "            \"model_response\": generated_text\n",
    "        }\n",
    "\n",
    "    def create_sql_prompt_and_response(self, sample):\n",
    "        full_prompt = self.TEXT2SQL_INFERENCE_PROMPT_TEMPLATE.format(\n",
    "            system_message=self.SYSTEM_MESSAGE,\n",
    "            input=sample[\"question\"],\n",
    "            context=sample[\"context\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"full_prompt\": full_prompt,\n",
    "            \"ground_truth\": sample[\"answer\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6925,
     "status": "ok",
     "timestamp": 1740828745461,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "NqJVESx8GP5U",
    "outputId": "dde5f61f-8995-4935-8f06-e7531445c89d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\",\n",
      "    \"ground_truth\": \"SELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso'\",\n",
      "    \"model_response\": \"### Instrucciones: Eres un modelo especializado en convertir lenguaje natural a SQL. Dada una pregunta en lenguaje natural y un contexto opcional de base de datos (esquema de tablas, tipos de datos, etc.), genera una consulta SQL válida y eficiente. Asegúrate de usar las columnas y tablas correctas, y evita consultas innecesariamente complejas.Responde solo con la consulta SQL y no incluyas explicaciones adicionales. ### Pregunta: ¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'? ### Contexto: CREATE TABLE rpt_actual_casos ( IdCaso INT NOT NULL, idTicket BIGINT NULL, idCliente BIGINT NULL, canal VARCHAR(45) NULL, estadoTicket INT NULL, Estado VARCHAR(50) NULL, frecepcion TIMESTAMP NULL, fgestion TIMESTAMP NULL, fechaEjecutivo TIMESTAMP NULL, fechaTicket TIMESTAMP NULL, fechaCierreRequerimiento TIMESTAMP NULL, tiempoRespuesta_min INT NULL, nombre_area VARCHAR(200) NULL, nombre_categoria VARCHAR(500) NULL, descripcionProceso VARCHAR(100) NULL, descripcionSubProceso VARCHAR(100) NULL, mensajeTicket TEXT NULL, scriptFase NVARCHAR(100) NULL, fechaAsignacion TIMESTAMP NULL, fechaResolucion TIMESTAMP NULL, mensajeResolucion TEXT NULL, nombreCliente VARCHAR(136) NOT NULL, descripcionGrupo VARCHAR(100) NULL, PNR VARCHAR(50) NULL, Usuario_Creacion VARCHAR(50) NULL, Usuario_Cierre VARCHAR(50) NULL ) ### Respuesta:\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "generador = Generador(model, tokenizer)\n",
    "resultado = generador.generate_sample(sql_dataset['test'][10])\n",
    "print(json.dumps(resultado, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ys-R2mIGP5U"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "training_args = TrainingArguments(\n",
    " per_device_train_batch_size=4,\n",
    " gradient_accumulation_steps=4,\n",
    " gradient_checkpointing=True,\n",
    " max_grad_norm= 0.3,\n",
    " ##num_train_epochs=5,\n",
    " max_steps=300,\n",
    " learning_rate=5e-5,\n",
    " save_total_limit=3,\n",
    " logging_steps=10,\n",
    " weight_decay=0.1,\n",
    " output_dir=\"/content/drive/MyDrive/IA/outputs/model/t5_text2sql_model_v1\",\n",
    " optim=\"adamw_torch\",\n",
    " lr_scheduler_type=\"cosine\",\n",
    " eval_strategy=\"steps\",\n",
    " eval_steps=100,\n",
    " warmup_ratio=0.05,\n",
    " fp16=False,\n",
    " report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8rmZHvAGP5V"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=sql_dataset[\"train\"],\n",
    "    eval_dataset=sql_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2310,
     "status": "ok",
     "timestamp": 1740828749389,
     "user": {
      "displayName": "Ariel Graverán",
      "userId": "11126964050768709715"
     },
     "user_tz": 180
    },
    "id": "q0uW_FuUK7zm",
    "outputId": "7d025025-7997-4a14-fc76-a8b63a598182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-uVhNF9UtPi"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkjZ1bjX6k28"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "trainer.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14Va4N7rcw-l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "6UyzPZ3XGP5V",
    "outputId": "12ca3b24-1662-42d4-f7d0-046a88c1d942"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/300 04:00 < 08:04, 0.41 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 322/1000 01:47 < 03:47, 2.97 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWXUgR2YmPYr"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/IA/outputs/model/t5_text2sql_model_v1\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/IA/outputs/model/t5_text2sql_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LCSkEMLM07Nc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# Directorios de modelo y tokenizer\n",
    "model_dir = \"./outputs/model/t5_v1/checkpoint-600\"\n",
    "tokenizer_dir = \"./outputs/model/t5_v1/checkpoint-600\"\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "# Mover el modelo a GPU si está disponible\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eYyn3TTV1Vxr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"¿Cuál es el promedio de tiempo de respuesta en minutos para los casos en estado 'En Proceso'?\",\n",
      "    \"ground_truth\": \"SELECT AVG(tiempoRespuesta_min) FROM rpt_actual_casos WHERE Estado = 'En Proceso'\",\n",
      "    \"model_response\": \"Eres un modelo especializado en convertir lenguaje natural a SQL. Dado una pregunta y un contexto opcional de base de datos\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "generador = Generador(model, tokenizer)\n",
    "resultado = generador.generate_sample(sql_dataset['test'][10])\n",
    "print(json.dumps(resultado, indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
